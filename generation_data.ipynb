{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark install-jdk findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1Ad9yLG2MKm",
        "outputId": "e786d52b-828a-427b-c5ad-a86dca5c2de6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting install-jdk\n",
            "  Downloading install_jdk-1.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading install_jdk-1.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=92a3ef9c52b1b1d8dcd267f89cdffcfe8bcc9134a44ba66af1ed9c38cefe19fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: findspark, pyspark, install-jdk\n",
            "Successfully installed findspark-2.0.1 install-jdk-1.1.0 pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q6eijMmg1lD_"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, rand, expr, floor\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
        "import random\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание SparkSession\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Synthetic Data Generation\").getOrCreate()"
      ],
      "metadata": {
        "id": "tKmEsISp1pzT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры генерации данных\n",
        "num_rows = 1000  # Количество строк (можно изменить по необходимости)\n",
        "products_list = [\"Laptop\", \"Smartphone\", \"Headphones\", \"Tablet\", \"Smartwatch\"]  # Список возможных товаров"
      ],
      "metadata": {
        "id": "zyIzZjae1qij"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для генерации случайной даты в пределах последнего года\n",
        "def random_date():\n",
        "    start_date = datetime.now() - timedelta(days=365)\n",
        "    random_days = random.randint(0, 365)\n",
        "    return (start_date + timedelta(days=random_days)).date()"
      ],
      "metadata": {
        "id": "uXB6emz11qrg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерация синтетических данных\n",
        "data = []\n",
        "for _ in range(num_rows):\n",
        "    date = random_date()\n",
        "    user_id = random.randint(1, 100)  # Случайный идентификатор пользователя\n",
        "    product = random.choice(products_list)\n",
        "    quantity = random.randint(1, 5)  # Случайное количество\n",
        "    price = round(random.uniform(10, 500), 2)  # Случайная цена в пределах от 10 до 500\n",
        "    data.append((date, user_id, product, quantity, price))"
      ],
      "metadata": {
        "id": "a5tZZYXv1rAc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Определение схемы DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"Date\", DateType(), True),\n",
        "    StructField(\"UserID\", IntegerType(), True),\n",
        "    StructField(\"Product\", StringType(), True),\n",
        "    StructField(\"Quantity\", IntegerType(), True),\n",
        "    StructField(\"Price\", DoubleType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "W5b9LYC11rJO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание DataFrame\n",
        "orders_df = spark.createDataFrame(data, schema)\n",
        "orders_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boMR9wQr1rSL",
        "outputId": "3c8d6c95-099f-4f7e-f3ee-52969e39bff1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Date: date, UserID: int, Product: string, Quantity: int, Price: double]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение DataFrame в формате CSV\n",
        "output_path = \"orders_data.csv\"\n",
        "orders_df.write.csv(output_path, header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "y7qZ1A2p1raf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Завершение сессии Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "C1viYLJr1rjP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YS4B-SBx2RJl"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}